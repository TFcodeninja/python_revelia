import os
import requests
from dotenv import load_dotenv
from PIL import Image
from whisper_transcribe import get_text_from_audio
from mistral_prompt_generator import generate_prompt

# Charger les variables dâ€™environnement (.env)
load_dotenv()

def generer_image(prompt, api_key, fichier_sortie="reve_genere.png"):
    """
    Envoie un prompt Ã  lâ€™API ClipDrop et enregistre lâ€™image gÃ©nÃ©rÃ©e.
    """
    url = "https://clipdrop-api.co/text-to-image/v1"
    headers = {
        "x-api-key": api_key
    }
    files = {
        "prompt": (None, prompt)
    }

    response = requests.post(url, headers=headers, files=files)

    if response.status_code == 200:
        with open(fichier_sortie, "wb") as f:
            f.write(response.content)
        print(f"âœ… Image gÃ©nÃ©rÃ©e : {fichier_sortie}")
    else:
        print(f"âŒ Erreur {response.status_code} : {response.text}")

def pipeline_audio_vers_image(audio_path):
    """
    Prend un fichier audio, gÃ©nÃ¨re une transcription, transforme en prompt, gÃ©nÃ¨re une image.
    """
    # Ã‰tape 1 : Transcription avec Groq
    transcription = get_text_from_audio(audio_path)
    print("\nğŸ“ Transcription :", transcription)

    # Ã‰tape 2 : GÃ©nÃ©ration du prompt avec Mistral
    prompt = generate_prompt(transcription)
    print("\nğŸ¨ Prompt gÃ©nÃ©rÃ© :", prompt)

    # Ã‰tape 3 : GÃ©nÃ©ration dâ€™image avec ClipDrop
    api_key = os.getenv("CLIPDROP_API_KEY")
    chemin_image = "reve_genere.png"
    generer_image(prompt, api_key, chemin_image)

    # Ã‰tape 4 : Affichage automatique de lâ€™image
    image = Image.open(chemin_image)
    image.show()
